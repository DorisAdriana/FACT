{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the reproduction of the paper Explainable Fairness in Recommendation\n",
    "\n",
    "Before running this code the dataset from amazon is needed. To do that you have to download the gz file from either\\\n",
    "https://drive.google.com/drive/folders/1cemAGIjESVeHrg4o6FEujdrj3zhOmYd2 (download reviews_electronics_filtered.json.gz)\\\n",
    "Or\\\n",
    "https://nijianmo.github.io/amazon/index.html (Download the 5-core electronics dataset)\n",
    "\n",
    "And extract it in input data.\n",
    "\n",
    "The first link is preferable as it will ensure the preprocessing step is faster but both should work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.args import *\n",
    "from scripts.base_model.preprocessing import *\n",
    "from scripts.base_model.train_base import *\n",
    "from scripts.base_model.models import *\n",
    "from scripts.evaluation.eval_model import *\n",
    "from scripts.evaluation.get_results import *\n",
    "from scripts.CEF_model.CEF_model import *\n",
    "from scripts.CEF_model.train_CEF import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args preprocessing\n",
    "\n",
    "sentires_dir    =       location of the preprocessed data. \\\n",
    "review_dir      =       location of the json dataset \\\n",
    "user_thresh     =       how many reviews a user needs to have \\\n",
    "item_thresh     =       how many reviews an item has to have \\\n",
    "sample_ratio    =       \\\n",
    "test_length     =       how many items in the test set. \\\n",
    "neg_length      =       amount of negative items. \\\n",
    "save_path       =       where the dataset object will be saved. \\\n",
    "user_pre        =       wether or not to use a pre created dataset. If this value is true it will use                     the data stroted in save_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser_preprocessing():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--sentires_dir\", dest=\"sentires_dir\", type=str, default=\"data/input_data/reviews_with_features.txt\", \n",
    "                        help=\"path to sentires data\")\n",
    "    parser.add_argument(\"--review_dir\", dest=\"review_dir\", type=str, default=\"data/input_data/reviews_Electronics_5_filtered.json\", \n",
    "                        help=\"path to original review data\")\n",
    "    parser.add_argument(\"--user_thresh\", dest=\"user_thresh\", type=int, default=20, \n",
    "                        help=\"remove users with reviews less than this threshold\")\n",
    "    parser.add_argument(\"--item_thresh\", dest=\"item_thresh\", type=int, default=10, \n",
    "                        help=\"remove users with reviews less than this threshold\")\n",
    "    parser.add_argument(\"--sample_ratio\", dest=\"sample_ratio\", type=int, default=2, \n",
    "                        help=\"the (negative: positive sample) ratio for training BPR loss\")\n",
    "    parser.add_argument(\"--test_length\", dest=\"test_length\", type=int, default=5, \n",
    "                        help=\"the number of test items\")\n",
    "    parser.add_argument(\"--neg_length\", dest=\"neg_length\", type=int, default=100, help=\"# of negative samples in evaluation\")\n",
    "    parser.add_argument(\"--save_path\", dest=\"save_path\", type=str, default=\"data/preprocessed_data/dataset.pickle\", \n",
    "                        help=\"The path to save the preprocessed dataset object\")\n",
    "    parser.add_argument(\"--use_pre\", dest=\"use_pre\", type=str, default=True, \n",
    "            help=\"Wether or not to use a stored dataset object\")\n",
    "    parser.add_argument(\"--extra_filter\", dest=\"extra_filter\", type=bool, default=False)\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args Train base model\n",
    "\n",
    "device = either cpu or cuda depending on wether you use a gpu \\\n",
    "batch_size = the batch size \\\n",
    "lr = learning rate \\\n",
    "rec_k = length of the recommendation list \\\n",
    "weight_decay = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser_training():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", dest = \"device\", type=str, default='cpu')\n",
    "    parser.add_argument(\"--gpu\", default=False)\n",
    "    parser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--rec_k\", dest=\"rec_k\", type=int, default=5, help=\"length of rec list\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0., type=float) # not sure whether to use\n",
    "    parser.add_argument(\"--model_path\", dest=\"model_path\", type=str, default=\"data/models/model.model\", \n",
    "                        help=\"The path to save the model\")\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--use_pre\", dest=\"use_pre\", type=str, default=False, \n",
    "            help=\"Wether or not to use a stored model object\")\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arg CEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "def arg_parser_CEF():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", dest = \"device\", type=str, default='cpu')\n",
    "    parser.add_argument(\"--rec_k\", dest=\"rec_k\", type=int, default=5, help=\"length of rec list\")\n",
    "    parser.add_argument(\"--ld\", default=1, type=float) # not sure whether to use\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--model_path\", dest=\"model_path\", type=str, default=\"data/models/CEF_model.model\", \n",
    "                        help=\"The path to save the model\")\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--use_pre\", dest=\"use_pre\", type=str, default=False, \n",
    "            help=\"Wether or not to use a stored model object\")\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser_results():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", dest = \"device\", type=str, default='cpu')\n",
    "    parser.add_argument(\"--remove_size\", dest=\"remove_size\", type=int, default=50)\n",
    "    parser.add_argument(\"--rec_k\", dest=\"rec_k\", type=int, default=5, help=\"length of rec list\")\n",
    "    parser.add_argument(\"--output_path\", dest=\"output_path\", type=str, default=\"results/result dicts/\", \n",
    "                        help=\"The path to save the model\")\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--beta\", dest=\"beta\", type=int, default=0.1 )\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the arguments for preprocessing\n",
    "preprocessing_args, unkown = arg_parser_preprocessing()\n",
    "# load dataset if the dataset exist\n",
    "dataset_path = preprocessing_args.save_path\n",
    "if preprocessing_args.use_pre:\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "else:\n",
    "    dataset = preprocessing(preprocessing_args)\n",
    "    with open(dataset_path, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  training loss:  0.61180735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:39<32:31, 39.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  training loss:  0.61180735 NDCG:  0.10359160643241823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [01:07<26:12, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:  training loss:  0.57026464\n",
      "epoch 2:  training loss:  0.5550149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:45<27:37, 35.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2:  training loss:  0.5550149 NDCG:  0.10381651090720209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [02:09<23:34, 30.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3:  training loss:  0.54434586\n",
      "epoch 4:  training loss:  0.5362154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [02:36<21:51, 29.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4:  training loss:  0.5362154 NDCG:  0.10294645533290998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:50<17:38, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss:  0.5291646\n",
      "epoch 6:  training loss:  0.5230428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [03:11<16:36, 23.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6:  training loss:  0.5230428 NDCG:  0.09993708388942679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [03:24<13:54, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7:  training loss:  0.5175618\n",
      "epoch 8:  training loss:  0.5130582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [03:42<13:13, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8:  training loss:  0.5130582 NDCG:  0.0973876936636239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [03:53<11:13, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9:  training loss:  0.5078671\n",
      "epoch 10:  training loss:  0.50394547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [04:10<10:51, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:  training loss:  0.50394547 NDCG:  0.0990166746045665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [04:21<09:28, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11:  training loss:  0.49937472\n",
      "epoch 12:  training loss:  0.4949719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [04:38<09:40, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12:  training loss:  0.4949719 NDCG:  0.0991456869293884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [04:50<08:47, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13:  training loss:  0.49052972\n",
      "epoch 14:  training loss:  0.48743987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [05:10<09:25, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14:  training loss:  0.48743987 NDCG:  0.09427988858864697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [05:21<08:19, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15:  training loss:  0.48307902\n",
      "epoch 16:  training loss:  0.4789293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [05:37<08:20, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16:  training loss:  0.4789293 NDCG:  0.09376936867881727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [05:49<07:26, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17:  training loss:  0.47556138\n",
      "epoch 18:  training loss:  0.4710593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [06:05<07:35, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18:  training loss:  0.4710593 NDCG:  0.09013579875675615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [06:17<06:58, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19:  training loss:  0.46673313\n",
      "epoch 20:  training loss:  0.46339512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [06:39<07:54, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20:  training loss:  0.46339512 NDCG:  0.08556640337986544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [06:52<07:10, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21:  training loss:  0.45866823\n",
      "epoch 22:  training loss:  0.4555082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [07:09<07:09, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22:  training loss:  0.4555082 NDCG:  0.09115541472132173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [07:23<06:38, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23:  training loss:  0.45150235\n",
      "epoch 24:  training loss:  0.44702834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [07:48<07:36, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24:  training loss:  0.44702834 NDCG:  0.08057509958049475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [08:18<08:40, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25:  training loss:  0.4429207\n",
      "epoch 26:  training loss:  0.4402961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [08:53<09:50, 25.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26:  training loss:  0.4402961 NDCG:  0.08874797685157249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [09:12<08:39, 23.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27:  training loss:  0.43440732\n",
      "epoch 28:  training loss:  0.43118733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [09:38<08:33, 24.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28:  training loss:  0.43118733 NDCG:  0.07910984194210952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [09:58<07:41, 23.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29:  training loss:  0.42946288\n",
      "epoch 30:  training loss:  0.42484823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [10:23<07:25, 23.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30:  training loss:  0.42484823 NDCG:  0.08533242685575217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [10:36<06:08, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31:  training loss:  0.42115334\n",
      "epoch 32:  training loss:  0.41740847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [10:54<05:35, 19.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32:  training loss:  0.41740847 NDCG:  0.0891506120264462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [11:06<04:40, 17.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33:  training loss:  0.41309103\n",
      "epoch 34:  training loss:  0.40800777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [11:23<04:19, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34:  training loss:  0.40800777 NDCG:  0.09584686616382503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [11:36<03:42, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35:  training loss:  0.40665892\n",
      "epoch 36:  training loss:  0.40183386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [11:55<03:40, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36:  training loss:  0.40183386 NDCG:  0.08758923568925032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [12:08<03:06, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37:  training loss:  0.39824852\n",
      "epoch 38:  training loss:  0.39497614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [12:26<03:00, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38:  training loss:  0.39497614 NDCG:  0.08235869217042267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [12:39<02:34, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39:  training loss:  0.3906127\n",
      "epoch 40:  training loss:  0.38598147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [13:01<02:36, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40:  training loss:  0.38598147 NDCG:  0.08334532391391446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [13:15<02:11, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41:  training loss:  0.38243368\n",
      "epoch 42:  training loss:  0.38124853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [13:36<02:04, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42:  training loss:  0.38124853 NDCG:  0.08448842580115631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [13:52<01:42, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43:  training loss:  0.37680104\n",
      "epoch 44:  training loss:  0.37277526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [14:13<01:31, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44:  training loss:  0.37277526 NDCG:  0.08802231188574748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [14:26<01:07, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45:  training loss:  0.36903256\n",
      "epoch 46:  training loss:  0.3659442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [14:49<00:55, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46:  training loss:  0.3659442 NDCG:  0.0832974601231487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [15:05<00:35, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47:  training loss:  0.3631826\n",
      "epoch 48:  training loss:  0.35982734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [15:27<00:19, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48:  training loss:  0.35982734 NDCG:  0.08558988137233568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [15:44<00:00, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49:  training loss:  0.35817054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_args, _ = arg_parser_training()\n",
    "model_path = train_args.model_path\n",
    "if train_args.use_pre:\n",
    "    base_model = BaseRecModel(dataset.feature_num, dataset).to(device)\n",
    "    base_model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    base_model = trainmodel(train_args, dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg : 0.08762500995027123\n",
      "f1 : 0.08410886742756805\n"
     ]
    }
   ],
   "source": [
    "ndcg, f1, _ = eval_model(dataset, 5, base_model, device)\n",
    "print(f\"ndcg : {ndcg}\")\n",
    "print(f\"f1 : {f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12564217\n",
      "epoch 0\n",
      "Disparity: 0.6790000200271606\n",
      "loss: 0.4620000123977661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [04:23<7:15:16, 263.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long tail rate: 0.44811237928007025\n",
      "12564217\n"
     ]
    }
   ],
   "source": [
    "## stil to do.\n",
    "CEF_args, _ = arg_parser_CEF()\n",
    "model_path = CEF_args.model_path\n",
    "CEF_model = CEF(CEF_args, dataset, base_model).to(device)\n",
    "if CEF_args.use_pre:\n",
    "    CEF_model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    CEF_model = train_delta(CEF_args, CEF_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a list of feature IDs, ranked by explainability score according to the trained CEF model (to clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2617 [00:12<3:02:14,  4.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults/ranked_ids.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 7\u001b[0m         ranked_ids \u001b[39m=\u001b[39m CEF_model\u001b[39m.\u001b[39;49mtop_k()\n\u001b[0;32m      8\u001b[0m         pickle\u001b[39m.\u001b[39mdump(ranked_ids, f)\n",
      "File \u001b[1;32mc:\\Users\\karst\\OneDrive\\Documenten\\uni\\master ai\\year 1\\fact\\code\\FACT_group_5\\scripts\\CEF_model\\CEF_model.py:163\u001b[0m, in \u001b[0;36mCEF.top_k\u001b[1;34m(self, beta)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtrange(delta\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]): \u001b[39m#delta.shape[1]\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     prox \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(delta[:,i])\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m--> 163\u001b[0m     validity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidity(i)\n\u001b[0;32m    165\u001b[0m     ES_scores[i] \u001b[39m=\u001b[39m validity \u001b[39m-\u001b[39m beta \u001b[39m*\u001b[39m prox\n\u001b[0;32m    166\u001b[0m \u001b[39m# sort ES_scores list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karst\\OneDrive\\Documenten\\uni\\master ai\\year 1\\fact\\code\\FACT_group_5\\scripts\\CEF_model\\CEF_model.py:145\u001b[0m, in \u001b[0;36mCEF.validity\u001b[1;34m(self, feature_id, k)\u001b[0m\n\u001b[0;32m    143\u001b[0m adjusted_uf_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39muser_feature_matrix\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    144\u001b[0m adjusted_uf_matrix[:,feature_id] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delta_u[:,feature_id]\n\u001b[1;32m--> 145\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_recommendations(adjusted_if_matrix, adjusted_uf_matrix)\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_exposures()\n\u001b[0;32m    148\u001b[0m og_exp0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_exposure[\u001b[39m\"\u001b[39m\u001b[39mG0\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\karst\\OneDrive\\Documenten\\uni\\master ai\\year 1\\fact\\code\\FACT_group_5\\scripts\\CEF_model\\CEF_model.py:77\u001b[0m, in \u001b[0;36mCEF.update_recommendations\u001b[1;34m(self, item_feature_matrix, user_feature_matrix, k)\u001b[0m\n\u001b[0;32m     75\u001b[0m user_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([user_feature_matrix[user] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(items))])\n\u001b[0;32m     76\u001b[0m item_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([item_feature_matrix[item] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items])\n\u001b[1;32m---> 77\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasemodel(torch\u001b[39m.\u001b[39;49mfrom_numpy(user_features)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice),\n\u001b[0;32m     78\u001b[0m                         torch\u001b[39m.\u001b[39;49mfrom_numpy(item_features)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\u001b[39m.\u001b[39msqueeze() \u001b[39m# het zijn 106 items ipv 105\u001b[39;00m\n\u001b[0;32m     79\u001b[0m scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(scores\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     80\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(scores)[\u001b[39m-\u001b[39mk:] \u001b[39m# indices (0-105) of items in the top k recs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karst\\anaconda3\\envs\\FACT_CPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\karst\\OneDrive\\Documenten\\uni\\master ai\\year 1\\fact\\code\\FACT_group_5\\scripts\\base_model\\models.py:23\u001b[0m, in \u001b[0;36mBaseRecModel.forward\u001b[1;34m(self, user_feature, item_feature)\u001b[0m\n\u001b[0;32m     21\u001b[0m fusion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultiply(user_feature, item_feature) \u001b[39m# USED TO BE NP, check base_train\u001b[39;00m\n\u001b[0;32m     22\u001b[0m fusion \u001b[39m=\u001b[39m fusion\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 23\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(fusion)\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\karst\\anaconda3\\envs\\FACT_CPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\karst\\anaconda3\\envs\\FACT_CPU\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\karst\\anaconda3\\envs\\FACT_CPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\karst\\anaconda3\\envs\\FACT_CPU\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "usepre = False\n",
    "if usepre:\n",
    "    with open(\"results/ranked_ids.pickle\", \"rb\") as f:\n",
    "        ranked_ids = pickle.load(f)\n",
    "else:\n",
    "    with open(\"results/ranked_ids.pickle\", \"wb\") as f:\n",
    "        ranked_ids = CEF_model.top_k()\n",
    "        pickle.dump(ranked_ids, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_args, _ = arg_parser_results()\n",
    "with open(\"results/ranked_ids.pickle\", \"rb\") as f:\n",
    "    CEF_delete_list = pickle.load(f)\n",
    "results = get_results(dataset, result_args, base_model,CEF_model, CEF_delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    for method in results:\n",
    "        result = results[method]\n",
    "        plt.plot(result[\"lt\"], result[\"ndcg\"], label = method)\n",
    "    \n",
    "    plt.xlabel(\"long tail rate\")\n",
    "    plt.ylabel(\"NDCG\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FACT_CPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c342e54a98a71db42fbd6ae012ac49d71548558fbe3a8689fd9084286f363d46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
